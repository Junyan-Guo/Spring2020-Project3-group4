{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GR5243-Project3 - Group4 - Advanced Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "03Yof9OEehEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyreadr\n",
        "!pip install PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UhpkJh7VdEzD",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import xgboost as xgb\n",
        "import pyreadr\n",
        "import scipy.io as scio\n",
        "from collections import OrderedDict\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials \n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from scipy.io import loadmat\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics \n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV \n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import scale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH0ItTRznD48",
        "colab_type": "text"
      },
      "source": [
        "# ADVANCED MODEL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzF5KwxOW7Op",
        "colab_type": "text"
      },
      "source": [
        "# Instruction\n",
        "1. Upload training data file in the google drive\n",
        "2. Get shareable link of the data file\n",
        "3. Get file ID (the file ID can be obstained from the link.) \n",
        "4. replace the file ID in corresponding code. (Detailed instruction also come with the code throughout the file.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ_ygrd0_2ED",
        "colab_type": "text"
      },
      "source": [
        "##Part 0: set up control and work directories, extract paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwm6XqcbYoW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####Authenticate the google drive account\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZIdIkO8aE0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV #Perforing grid search \n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline\n",
        "from matplotlib.pylab import rcParams\n",
        "rcParams['figure.figsize'] = 12, 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTPSVPz2AGOr",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: Import Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENSgnxsnaY6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####get the file shareable link = https://drive.google.com/open?id=1oliwM2-sH8CD_3q3yUbLF836U9A1-Lc0\n",
        "#####The file ID is the letter after \"id=\"."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-URgQLzAT5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####please replace the id of your file\n",
        "download = drive.CreateFile({'id': '1oliwM2-sH8CD_3q3yUbLF836U9A1-Lc0'}) \n",
        "download.GetContentFile('train_set.zip')\n",
        "!unzip train_set.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmiwDIdZ-ArZ",
        "colab_type": "code",
        "outputId": "0d81301b-3c11-4f9b-e269-ec0f955cebb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#####Run the code, and go to the URL in th output, enter the authorization code, done\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEsCWzFjHIPA",
        "colab_type": "text"
      },
      "source": [
        "# I. Our Advanced Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ozniXAXBdEXB"
      },
      "source": [
        "We are using PCA with Bagging-SVM as our Advacned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTwuJu3Snw_C",
        "colab_type": "text"
      },
      "source": [
        "Notation on These functions:\n",
        "1. extract_mat(): \n",
        "\n",
        "  **TAKES IN** a list returned by a loadmat function. \n",
        "\n",
        "  **RETURN** an array that have all the points in the mat file.\n",
        "\n",
        "2. get_f(): \n",
        "\n",
        "  **TAKES IN** a direction that contains a _single_ .mat file. \n",
        "\n",
        "  **RETURN** an ndarray contains the pairwise euclidian distance between the coordinate contains in the .mat file.\n",
        "\n",
        "3. feature_extraction(): \n",
        "\n",
        "  **TAKES IN** a direction that contains the direction that contains _all_ the .mat file for the train. \n",
        "  \n",
        "  **RETURNS** a ndarray contains the train_x with features set as pairwise euclidian distance between the coordinate contains in the .mat file.\n",
        "\n",
        "4. f_pca(): \n",
        "\n",
        "  **TAKES IN** a ndarray contains all the train_x.\n",
        "\n",
        "  **RETURNS** a ndarray contains decomposed x and the decompositon model.\n",
        "\n",
        "5. BaggingSVM_w_pca(): \n",
        "\n",
        "  **TAKES IN** two ndarrays as train_x(without decomposition) and train_y. \n",
        "  \n",
        "  **RETURNS** the SVM-Bagging model trained with decomposed-train_x and train_y.\n",
        "\n",
        "6. claim_possible_acc_BSVM(): \n",
        "\n",
        "  **TAKES IN** three arguments which is the direction that contains _all_the .mat file for x, the direction that contains a file named *label.csv* that have a column named as emotion_idx as the train_y. \n",
        "  \n",
        "  **RETURNS** the possible accuracy of the Logistic-Bagging model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T7TYRWAadEDV",
        "colab": {}
      },
      "source": [
        "def extract_mat(x):\n",
        "    v = list(x.keys())[-1]\n",
        "    return x[v]\n",
        "\n",
        "def get_f(file_dir):\n",
        "    '''Argument: \n",
        "        file_dir: The whole direction contain the exact mat file\n",
        "       \n",
        "       Return:\n",
        "        a np.array contains the featrues of single X'''\n",
        "    a = extract_mat(loadmat(file_dir))\n",
        "    b = cdist(a, a)\n",
        "    r = b[np.triu_indices(b.shape[1], 1)].flatten()\n",
        "    return r\n",
        "\n",
        "def f_pca(x):\n",
        "    my_pca = PCA(n_components = 130)\n",
        "    new_X = my_pca.fit_transform(x)\n",
        "    compo = sum(my_pca.explained_variance_ratio_)*100\n",
        "    print(f'The Decomposition take up {compo: 0.4f}% Information of original Data')\n",
        "    \n",
        "    return new_X, my_pca\n",
        "\n",
        "def feature_extraction(dir_x):\n",
        "    if (dir_x[-1] != '/'):\n",
        "        dir_x = dir_x + '/'\n",
        "    \n",
        "    fea_start = time.time()\n",
        "    \n",
        "    filenames = list(os.listdir(dir_x))\n",
        "    filenames.sort()\n",
        "    X = np.array(list(map(get_f, ((dir_x + i) for i in filenames))))\n",
        "    \n",
        "    fea_end = time.time()\n",
        "    fea_time = fea_end - fea_start\n",
        "    \n",
        "    print('Feature Extraction Completed!')\n",
        "    print(f'Feature Extraction Cost: {fea_time: 0.2f} Seconds')\n",
        "    return X\n",
        "\n",
        "def BaggingSVM_w_pca(train_X, train_y):\n",
        "    \n",
        "    train_X, pca_mode = f_pca(train_X)    \n",
        "    \n",
        "    start_SVM = time.time()\n",
        "    S_svm = SVC(C = 0.1,\n",
        "                kernel = 'linear',\n",
        "                shrinking = True,\n",
        "                decision_function_shape = 'ovo')\n",
        "    Bagg_SVM = BaggingClassifier(S_svm,\n",
        "                                 n_estimators = 80,\n",
        "                                 n_jobs = 5,\n",
        "                                 bootstrap_features = True)\n",
        "    Bagg_SVM.fit(train_X, train_y)\n",
        "    end_SVM = time.time()\n",
        "    \n",
        "    Train_time = end_SVM - start_SVM\n",
        "    print(f'The Time for train is: {Train_time: 0.2f} Seconds')\n",
        "    return Bagg_SVM, pca_mode\n",
        "\n",
        "\n",
        "def claim_possible_acc_BSVM(X_path, y_path, n_iter = 1):\n",
        "    X = feature_extraction(X_path)\n",
        "    y = pd.read_csv(y_path).emotion_idx\n",
        "    \n",
        "    accs = []\n",
        "    for i in range(n_iter):\n",
        "        trainx, testx, trainy, testy = train_test_split(X, y, test_size = .2)\n",
        "        model, pca_mode= BaggingSVM_w_pca(trainx, trainy)\n",
        "        new_testx = pca_mode.transform(testx)\n",
        "        testy_hat = model.predict(new_testx)\n",
        "        accs.append(accuracy_score(testy, testy_hat))\n",
        "    ret = np.mean(accs)*100\n",
        "    return print(f'Our model should have about {ret: 0.4f}% accuracy')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8f04b477-8e45-40e8-9b98-d80b883e0cf1",
        "id": "-8oC03XcdDuA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "# This line can output the Claimed Accuracy\n",
        "# You don't really need run it\n",
        "claim_possible_acc_BSVM('train_set/points', 'train_set/label.csv', 15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature Extraction Completed!\n",
            "Feature Extraction Cost:  0.89 Seconds\n",
            "The Decomposition take up  99.9079% Information of original Data\n",
            "The Time for train is:  132.73 Seconds\n",
            "The Decomposition take up  99.9084% Information of original Data\n",
            "The Time for train is:  133.39 Seconds\n",
            "The Decomposition take up  99.9082% Information of original Data\n",
            "The Time for train is:  150.10 Seconds\n",
            "The Decomposition take up  99.9053% Information of original Data\n",
            "The Time for train is:  150.06 Seconds\n",
            "The Decomposition take up  99.9091% Information of original Data\n",
            "The Time for train is:  141.05 Seconds\n",
            "The Decomposition take up  99.9090% Information of original Data\n",
            "The Time for train is:  126.75 Seconds\n",
            "The Decomposition take up  99.9083% Information of original Data\n",
            "The Time for train is:  145.79 Seconds\n",
            "The Decomposition take up  99.9084% Information of original Data\n",
            "The Time for train is:  148.31 Seconds\n",
            "The Decomposition take up  99.9078% Information of original Data\n",
            "The Time for train is:  139.24 Seconds\n",
            "The Decomposition take up  99.9133% Information of original Data\n",
            "The Time for train is:  142.79 Seconds\n",
            "The Decomposition take up  99.9082% Information of original Data\n",
            "The Time for train is:  125.05 Seconds\n",
            "The Decomposition take up  99.9081% Information of original Data\n",
            "The Time for train is:  125.84 Seconds\n",
            "The Decomposition take up  99.9074% Information of original Data\n",
            "The Time for train is:  148.81 Seconds\n",
            "The Decomposition take up  99.9096% Information of original Data\n",
            "The Time for train is:  133.39 Seconds\n",
            "The Decomposition take up  99.9078% Information of original Data\n",
            "The Time for train is:  147.61 Seconds\n",
            "Our model should have about  52.2533% accuracy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRVnsbA5I8Tp",
        "colab_type": "text"
      },
      "source": [
        "After doing some test, we can claim that Our Advanced Model would have 52.25% Accuracy.\n",
        "\n",
        "The train time for our Advanced Model is about 110 seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qj1AkgVb-nU",
        "colab_type": "text"
      },
      "source": [
        "You can use The Code below to train the model on the whole train data set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQvTNqynH6pS",
        "colab_type": "code",
        "outputId": "2f2dd3a3-4183-4f0e-8921-28b69c4c7419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X = feature_extraction('train_set/points')\n",
        "y = pd.read_csv('train_set/label.csv').emotion_idx\n",
        "advanced_model, pca_sub_model = BaggingSVM_w_pca(X, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature Extraction Completed!\n",
            "Feature Extraction Cost:  0.82 Seconds\n",
            "The Decomposition take up  1.00% Information of original Data\n",
            "The Time for train is:  322.94 Seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr5VMoOHcHlY",
        "colab_type": "text"
      },
      "source": [
        "Then, you can use the code below to test on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUTxoP3-cNIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_test = feature_extraction('''your test set direction''')\n",
        "# X_test_decomp = pca_sub_model.transform(X_test)\n",
        "# y_predict = advanced_model.predict(X_test_decomp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLveocLFm731",
        "colab_type": "text"
      },
      "source": [
        "# II. XGBOOST Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq0J8Mi-AS1K",
        "colab_type": "text"
      },
      "source": [
        "## Part 0: Feature Extration and Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq6b4c4qnCUY",
        "colab_type": "code",
        "outputId": "3c852f70-e4e3-4f18-e31e-f34d79df0430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##### Importing the fidusial points \n",
        "import scipy.io as scio\n",
        "from collections import OrderedDict \n",
        "points_path = 'train_set/points'\n",
        "points = [p for p in sorted(os.listdir(points_path))]\n",
        "all_points = []\n",
        "for p in points:\n",
        "  poiFile = os.path.join(points_path, p)\n",
        "  poi = scio.loadmat(poiFile)\n",
        "  poi = OrderedDict(poi)\n",
        "  all_points.append(poi.popitem()[1])\n",
        "y = pd.read_csv('train_set/label.csv')['emotion_idx']\n",
        "\n",
        "print('success')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "success\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCG5GPOHBFEE",
        "colab_type": "code",
        "outputId": "c43d9a46-7e4b-4cb5-df31-81b7d18ec77d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##### Calculating pairwise distance \n",
        "pair_dist = []\n",
        "for i in range(len(all_points)):\n",
        "  pair_dist.append(metrics.pairwise_distances(all_points[i])[np.triu_indices(78)])\n",
        "\n",
        "##### Split train_set & test_set \n",
        "points_train, points_test, y_train, y_test = train_test_split(pair_dist, y, random_state=42, test_size=0.2)\n",
        "print('success')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "success\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyK2LBY7QhLa",
        "colab_type": "code",
        "outputId": "0f6925d0-0542-4724-a032-779ec0b600b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "##### Feature Extration/Calculating pairwise distance and the time for feature extration \n",
        "import time\n",
        "\n",
        "allpoints_train, allpoints_test, y_train, y_test = train_test_split(all_points, y, random_state=42, test_size=0.2)\n",
        "print('success')\n",
        "\n",
        "\n",
        "train_pair_dist = []\n",
        "for i in range(len(allpoints_train)):\n",
        "  pair_dist.append(metrics.pairwise_distances(allpoints_train[i])[np.triu_indices(78)])\n",
        "\n",
        "test_pair_dist = []\n",
        "for i in range(len(allpoints_test)):\n",
        "  pair_dist.append(metrics.pairwise_distances(allpoints_test[i])[np.triu_indices(78)])\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "pair_dist = []\n",
        "for i in range(len(all_points)):\n",
        "  pair_dist.append(metrics.pairwise_distances(all_points[i])[np.triu_indices(78)])\n",
        "finish = time.time()\n",
        "print(\"Time on feature selection done in %0.3fs\" % (finish-start))\n",
        "\n",
        "start = time.time()\n",
        "train_pair_dist = []\n",
        "for i in range(len(allpoints_train)):\n",
        "  pair_dist.append(metrics.pairwise_distances(allpoints_train[i])[np.triu_indices(78)])\n",
        "finish = time.time()\n",
        "print(\"Time on feature selection training set done in %0.3fs\" % (finish-start))\n",
        "\n",
        "start = time.time()\n",
        "test_pair_dist = []\n",
        "for i in range(len(allpoints_test)):\n",
        "  pair_dist.append(metrics.pairwise_distances(allpoints_test[i])[np.triu_indices(78)])\n",
        "finish = time.time()\n",
        "print(\"Time on feature selection test set done in %0.3fs\" % (finish-start))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "success\n",
            "Time on feature selection done in 1.098s\n",
            "Time on feature selection training set done in 0.922s\n",
            "Time on feature selection test set done in 0.277s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQem6giVAmn5",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: XGBoost Training \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUY6cPJqBaxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCby3APqBIfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modelfit(alg, dtrain, predictors, cv_folds=10):\n",
        "  #Fit the algorithm on the data\n",
        "  alg.fit(dtrain, predictors)\n",
        "\n",
        "  #Predict training set:\n",
        "  dtrain_predictions = alg.predict(dtrain)\n",
        "  dtrain_predprob = alg.predict_proba(dtrain)[:,1]\n",
        "\n",
        "  #Print model report:\n",
        "  print(\"\\nModel Report\")\n",
        "  print(\"Accuracy : %.4g\" % metrics.accuracy_score(predictors, dtrain_predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3aXpnJNAyhc",
        "colab_type": "text"
      },
      "source": [
        "## Part 2:XGBoost Default setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WWmYy_vBODP",
        "colab_type": "code",
        "outputId": "b2260975-5bab-473c-ac31-4c8669161f64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "####XGBOOST base model with default setting\n",
        "start = time.time()\n",
        "xgb_base = XGBClassifier(\n",
        " objective= 'multi:softmax',\n",
        " num_class= 22,\n",
        " seed=1000)\n",
        "\n",
        "modelfit(xgb_base, np.array(points_train), np.array(y_train))\n",
        "finish = time.time()\n",
        "print(\"Prediction on train_set done in %0.3fs\" % (finish-start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model Report\n",
            "Accuracy : 1\n",
            "Prediction on train_set done in 807.874s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mmw-u4ZE_m6",
        "colab_type": "code",
        "outputId": "cb1a54ca-dea8-4221-ce4d-698cd60c921a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "start = time.time()\n",
        "preds = xgb_base.predict(points_test)\n",
        "acc_preds = metrics.accuracy_score(preds, y_test)\n",
        "finish = time.time()\n",
        "print(\"Prediction on test_set done in %0.3fs\" % (finish - start))\n",
        "print(\"Test_set accurarcy is %0.3f\" %acc_preds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction on test_set done in 0.740s\n",
            "Test_set accurarcy is 0.482\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJkm2IK6A5zv",
        "colab_type": "text"
      },
      "source": [
        "### Tuning Process(comment it out because the process is time comsuming)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X630cmBQdNIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####tune hyperparameter\n",
        "\n",
        "## tune the max_depth and min_child_weight parameter\n",
        "# param_test1 = { \n",
        "#  'max_depth': range(4,5,6),\n",
        "#  'min_child_weight': range(4,5,6)\n",
        "# } \n",
        "# gsearch1 = GridSearchCV(xgb_base, param_grid = param_test1, scoring ='accuracy', cv = 5)\n",
        "# gsearch1.fit(np.array(points_train), np.array(y_train))\n",
        "# best_parameters1 = gsearch1.best_estimator_.get_params()\n",
        "# for param_name in sorted(param_test1.keys()):\n",
        "#     print(\"\\t%s: %r\" % (param_name, best_parameters1[param_name]))\n",
        "\n",
        "## use the best_parameter above to xgb2\n",
        "# start = time.time()\n",
        "# xgb2 = XGBClassifier(\n",
        "#  objective= 'multi:softmax',\n",
        "#  num_class= 22,\n",
        "#  max_depth=4,\n",
        "#  min_child_weight=4,\n",
        "#  seed=1000)\n",
        "# modelfit(xgb2, np.array(points_train), np.array(y_train))\n",
        "# finish = time.time()\n",
        "# print(\"Prediction on train_set done in %0.3fs\" % (finish-start))\n",
        "# start = time.time()\n",
        "# preds = xgb2.predict(points_test)\n",
        "# acc_pred = metrics.accuracy_score(preds, y_test)\n",
        "# finish = time.time()\n",
        "# print(\"Prediction on test_set done in %0.3fs\" % (finish - start))\n",
        "# print(\"Test_set accurarcy is %0.3f\" %acc_pred)\n",
        "\n",
        "## However, the accuracy is lower than that of the base model, so we keep the same parameter \n",
        "## as before, and tune other parameters.\n",
        "\n",
        "## tune the gamma parameter\n",
        "# param_test2 = { \n",
        "#  'gamma':[i/10.0 for i in range(0,5)]\n",
        "# } \n",
        "# gsearch2 = GridSearchCV(xgb1, param_grid = param_test2, scoring ='accuracy', cv = 5)\n",
        "# gsearch2.fit(np.array(points_train), np.array(y_train))\n",
        "# best_parameters2 = gsearch2.best_estimator_.get_params()\n",
        "# for param_name in sorted(param_test2.keys()):\n",
        "#     print(\"\\t%s: %r\" % (param_name, best_parameters2[param_name]))\n",
        "\n",
        "# start = time.time()\n",
        "# xgb3 = XGBClassifier(\n",
        "#  objective= 'multi:softmax',\n",
        "#  num_class= 22,\n",
        "#  gamma=0.4,\n",
        "#  seed=1000)\n",
        "\n",
        "# modelfit(xgb3, np.array(points_train), np.array(y_train))\n",
        "# finish = time.time()\n",
        "# print(\"Prediction on train_set done in %0.3fs\" % (finish-start))\n",
        "\n",
        "# start = time.time()\n",
        "# preds = xgb3.predict(points_test)\n",
        "# acc_pred = metrics.accuracy_score(preds, y_test)\n",
        "# finish = time.time()\n",
        "# print(\"Prediction on test_set done in %0.3fs\" % (finish - start))\n",
        "# print(\"Test_set accurarcy is %0.3f\" %acc_pred)\n",
        "\n",
        "## However, the accuracy is lower than that of the base model, so we keep the same parameter \n",
        "## as before, and tune other parameters.\n",
        "\n",
        "## tune the subsample and colsample_bytree parameters\n",
        "#param_test = {\n",
        " #'subsample':[i/10.0 for i in range(6,10)],\n",
        " #'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
        "#}\n",
        "#gsearch = GridSearchCV(xgb_base, param_grid = param_test, scoring ='accuracy', cv = 5)\n",
        "#gsearch.fit(np.array(points_train), np.array(y_train))\n",
        "#best_parameters = gsearch.best_estimator_.get_params()\n",
        "#for param_name in sorted(param_test.keys()):\n",
        "    #print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
        "\n",
        "# start = time.time()\n",
        "# xgb4 = XGBClassifier(\n",
        "#  objective = 'multi:softmax',\n",
        "#  num_class = 22,\n",
        "#  seed = 1000,\n",
        "#  colsample_bytree=0.6,\n",
        "#  subsample=0.7)\n",
        "\n",
        "# modelfit(xgb4, np.array(points_train), np.array(y_train))\n",
        "# finish = time.time()\n",
        "# print(\"Prediction on train_set done in %0.3fs\" % (finish-start)) \n",
        "\n",
        "# start = time.time()\n",
        "# preds = xgb4.predict(points_test)\n",
        "# acc_pred = metrics.accuracy_score(preds, y_test)\n",
        "# finish = time.time()\n",
        "# print(\"Prediction on test_set done in %0.3fs\" % (finish - start))\n",
        "# print(\"Test_set accurarcy is %0.3f\" %acc_pred)\n",
        "\n",
        "## We use the best parameters above because the accuracy increases and the prediction time \n",
        "## decreases. Then, we tune other parameter based on the xgb4.\n",
        "\n",
        "##tune reg_alpha parameter\n",
        "#param_test4 = {\n",
        "#  'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
        "# }\n",
        "# gsearch4 = GridSearchCV(xgb4, param_grid = param_test4, scoring ='accuracy', cv = 5)\n",
        "# gsearch4.fit(np.array(points_train), np.array(y_train))\n",
        "# best_parameters4 = gsearch4.best_estimator_.get_params()\n",
        "# for param_name in sorted(param_test4.keys()):\n",
        "#     print(\"\\t%s: %r\" % (param_name, best_parameters4[param_name]))\n",
        "\n",
        "# start = time.time()\n",
        "# xgb5 = XGBClassifier(\n",
        "#  objective= 'multi:softmax',\n",
        "#  num_class= 22,\n",
        "#  seed=1000,\n",
        "#  colsample_bytree=0.7,\n",
        "#  subsample=0.6,\n",
        "#  reg_alpha=1)\n",
        "\n",
        "# modelfit(xgb5, np.array(points_train), np.array(y_train))\n",
        "# finish = time.time()\n",
        "# print(\"Prediction on train_set done in %0.3fs\" % (finish-start)) \n",
        "\n",
        "# start = time.time()\n",
        "# preds = xgb5.predict(points_test)\n",
        "# acc_pred = metrics.accuracy_score(preds, y_test)\n",
        "# finish = time.time()\n",
        "# print(\"Prediction on test_set done in %0.3fs\" % (finish - start))\n",
        "# print(\"Test_set accurarcy is %0.3f\" %acc_pred)\n",
        "\n",
        "##Since the best parameter of reg_alpha=1e-05, and the accuracy is so close to that of the former \n",
        "##model, we decide to use the xgb5 as our final model."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnCvoU1jBH0I",
        "colab_type": "text"
      },
      "source": [
        "## Part 3: The improved XGboost model after tuning the parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0m7v0Zi51es",
        "colab_type": "code",
        "outputId": "ad41b8d6-580f-4989-d198-d2bae5ca9d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "####XGBOOSTING improved model\n",
        "start = time.time()\n",
        "xgb5 = XGBClassifier(\n",
        " objective= 'multi:softmax',\n",
        " num_class= 22,\n",
        " seed=1000,\n",
        " colsample_bytree=0.6,\n",
        " subsample=0.7,\n",
        " reg_alpha=1)\n",
        "\n",
        "modelfit(xgb5, np.array(points_train), np.array(y_train))\n",
        "finish = time.time()\n",
        "print(\"Prediction on train_set done in %0.3fs\" % (finish-start)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model Report\n",
            "Accuracy : 0.9995\n",
            "Prediction on train_set done in 483.013s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQc4X1AharE8",
        "colab_type": "code",
        "outputId": "3e5ab507-4fe8-4e51-ed81-7dc8138e52e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "start = time.time()\n",
        "preds = xgb5.predict(points_test)\n",
        "acc_pred = metrics.accuracy_score(preds, y_test)\n",
        "finish = time.time()\n",
        "print(\"Prediction on test_set done in %0.3fs\" % (finish - start))\n",
        "print(\"Test_set accurarcy is %0.3f\" %acc_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction on test_set done in 0.770s\n",
            "Test_set accurarcy is 0.502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8xEcmPWngLi",
        "colab_type": "text"
      },
      "source": [
        "# III. BAGGING-LOG MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKng2id0gg4Y",
        "colab_type": "text"
      },
      "source": [
        "Notation on These functions:\n",
        "1. extract_mat(): \n",
        "\n",
        "  **TAKES IN** a list returned by a loadmat function. \n",
        "  \n",
        "  **RETURN** an array that have all the points in the mat file.\n",
        "\n",
        "2. get_f(): \n",
        "\n",
        "  **TAKES IN** a direction that contains a _single_ .mat file. \n",
        "  \n",
        "  **RETURN** an ndarray contains the pairwise euclidian distance between the coordinate contains in the .mat file.\n",
        "\n",
        "3. feature_extraction(): \n",
        "  \n",
        "  **TAKES IN** a direction that contains the direction that contains _all_ the .mat file for the train. \n",
        "  \n",
        "  **RETURNS** a ndarray contains the train_x with features set as pairwise euclidian distance between the coordinate contains in the .mat file.\n",
        "\n",
        "4. f_pca(): \n",
        "\n",
        "  **TAKES IN** a ndarray contains all the train_x. \n",
        "  \n",
        "  **RETURNS** a ndarray contains decomposed x and the decompositon model.\n",
        "\n",
        "5. BaggingLR_w_pca(): \n",
        "\n",
        "  **TAKES IN** two ndarrays as train_x(without decomposition) and train_y. \n",
        "  \n",
        "  **RETURNS** the Logistic-Bagging model trained with decomposed-train_x and train_y.\n",
        "\n",
        "6. claim_possible_acc_BL(): \n",
        "\n",
        "  **TAKES IN** three arguments which is the direction that contains _all_the .mat file for x, the direction that contains a file named *label.csv* that have a column named as emotion_idx as the train_y. \n",
        "  \n",
        "  **RETURNS** the possible accuracy of the Logistic-Bagging model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw02xXE6dKQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_mat(x):\n",
        "    v = list(x.keys())[-1]\n",
        "    return x[v]\n",
        "\n",
        "def get_f(file_dir):\n",
        "    '''Argument: \n",
        "        file_dir: The whole direction contain the exact mat file\n",
        "       \n",
        "       Return:\n",
        "        a np.array contains the featrues of single X'''\n",
        "    a = extract_mat(loadmat(file_dir))\n",
        "    b = cdist(a, a)\n",
        "    r = b[np.triu_indices(b.shape[1], 1)].flatten()\n",
        "    return r\n",
        "\n",
        "def feature_extraction(dir_x):\n",
        "    if (dir_x[-1] != '/'):\n",
        "        dir_x = dir_x + '/'\n",
        "    \n",
        "    fea_start = time.time()\n",
        "    \n",
        "    filenames = list(os.listdir(dir_x))\n",
        "    filenames.sort()\n",
        "    X = np.array(list(map(get_f, ((dir_x + i) for i in filenames))))\n",
        "    \n",
        "    fea_end = time.time()\n",
        "    fea_time = fea_end - fea_start\n",
        "    \n",
        "    print('Feature Extraction Completed!')\n",
        "    print(f'Feature Extraction Cost: {fea_time: 0.2f} Seconds')\n",
        "    return X\n",
        "\n",
        "def f_pca(x):\n",
        "    my_pca = PCA(n_components = 130)\n",
        "    new_X = my_pca.fit_transform(x)\n",
        "    compo = sum(my_pca.explained_variance_ratio_)*100\n",
        "    print(f'The Decomposition take up {compo: 0.2f}% Information of original Data')\n",
        "    \n",
        "    return new_X, my_pca\n",
        "\n",
        "def BaggingLR_w_pca(train_X, train_y):\n",
        "    \n",
        "    train_X, pca_mode = f_pca(train_X)    \n",
        "    \n",
        "    start_lr = time.time()\n",
        "    lr = LogisticRegression(C = 1,\n",
        "                            penalty = 'l2',\n",
        "                            fit_intercept = False)\n",
        "    Bag_lr = BaggingClassifier(lr,\n",
        "                               n_estimators = 70,\n",
        "                               n_jobs = 5,\n",
        "                               bootstrap_features = True,\n",
        "                               verbose = 7)\n",
        "    Bag_lr.fit(train_X, train_y)\n",
        "    end_lr = time.time()\n",
        "    \n",
        "    Train_time = end_lr - start_lr\n",
        "    print(f'The Time for train is: {Train_time: 0.2f} Seconds')\n",
        "    return Bag_lr, pca_mode\n",
        "\n",
        "\n",
        "def claim_possible_acc_BL(X_path, y_path, n_iter = 1):\n",
        "    X = feature_extraction(X_path)\n",
        "    y = pd.read_csv(y_path).emotion_idx\n",
        "    \n",
        "    accs = []\n",
        "    for i in range(n_iter):\n",
        "        trainx, testx, trainy, testy = train_test_split(X, y, test_size = .2)\n",
        "        model, pca_mode= BaggingLR_w_pca(trainx, trainy)\n",
        "        new_testx = pca_mode.transform(testx)\n",
        "        testy_hat = model.predict(new_testx)\n",
        "        accs.append(accuracy_score(testy, testy_hat))\n",
        "    ret = np.mean(accs)*100\n",
        "    return print(f'The Bagging-Logistic model should have about {ret: 0.4f}% accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN113ZfGeV4o",
        "colab_type": "code",
        "outputId": "f44c544a-a47f-4fd5-b533-7ef97f19638f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "claim_possible_acc_BL('train_set/points', 'train_set/label.csv',10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature Extraction Completed!\n",
            "Feature Extraction Cost:  0.88 Seconds\n",
            "The Decomposition take up  99.91% Information of original Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:   25.1s remaining:   37.7s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:   25.1s remaining:   16.8s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.8s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.8s finished\n",
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Time for train is:  25.76 Seconds\n",
            "The Decomposition take up  99.91% Information of original Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:   25.5s remaining:   38.3s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:   25.5s remaining:   17.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.7s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.7s finished\n",
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Time for train is:  25.66 Seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Decomposition take up  99.91% Information of original Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:   24.9s remaining:   37.4s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:   25.0s remaining:   16.7s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.3s finished\n",
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Time for train is:  25.30 Seconds\n",
            "The Decomposition take up  99.91% Information of original Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:   25.3s remaining:   38.0s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:   25.3s remaining:   16.9s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.8s finished\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.8s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.2s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Time for train is:  25.76 Seconds\n",
            "The Decomposition take up  99.91% Information of original Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:   25.0s remaining:   37.5s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:   25.1s remaining:   16.7s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.3s finished\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Time for train is:  25.27 Seconds\n",
            "The Decomposition take up  99.91% Information of original Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:   24.7s remaining:   37.0s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:   24.8s remaining:   16.5s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.4s finished\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Time for train is:  25.36 Seconds\n",
            "The Decomposition take up  99.91% Information of original Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:   24.9s remaining:   37.3s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:   25.0s remaining:   16.6s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.2s finished\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.2s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Time for train is:  25.17 Seconds\n",
            "The Decomposition take up  99.91% Information of original Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:   24.6s remaining:   36.9s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:   24.7s remaining:   16.5s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.3s finished\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Time for train is:  25.29 Seconds\n",
            "The Decomposition take up  99.91% Information of original Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:   24.2s remaining:   36.3s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:   24.3s remaining:   16.2s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.3s finished\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Time for train is:  25.28 Seconds\n",
            "The Decomposition take up  99.91% Information of original Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:   24.8s remaining:   37.2s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:   25.1s remaining:   16.7s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Time for train is:  25.42 Seconds\n",
            "The Bagging-Logistic model should have about  53.7200% accuracy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:   25.4s finished\n",
            "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
            "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s finished\n",
            "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvc02jnE8OeC",
        "colab_type": "text"
      },
      "source": [
        "The accuracy of Bagging-Logistic model may be a bit higher(53.6%) than our advanced model but after some testing, this model is not as stable as our advanced model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Go4pEclchdJ",
        "colab_type": "text"
      },
      "source": [
        "reference: https://www.cnblogs.com/wj-1314/p/10422159.html"
      ]
    }
  ]
}